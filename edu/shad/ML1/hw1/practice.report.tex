\documentclass[12pt]{article}

\usepackage{amsfonts,amssymb}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage[ruled, lined]{algorithm2e}
\usepackage{verbatim}
\usepackage{hyperref}

\textheight=220mm
\textwidth=160mm

\newcommand{\sgn}{\operatorname{sgn}}
\newcommand{\argmax}{\operatorname{argmax}}
\newcommand{\NA}{\operatorname{NA}}
\newcommand{\OR}{\operatorname{ or }}
\newcommand{\LCS}{\operatorname{LCS}}
%\DeclareMathOperator{\sgn}{sgn}

\title{\bf Практикум к домашнему заданию. \\ <<Машинное
Обучение>>}
\author{А.Е. Оразаев}
\date{}
\begin{document}

\voffset=-20mm
\hoffset=-12mm
\font\Got=eufm10 scaled\magstep2 \font\Got=eufm10

\maketitle

\section{Ход выполнения практикума.}

\begin{enumerate}
    \item Первым делом был написан EM-алгоритм точь-в-точь как в лекциях.

    \item Далее был реализован автоматический подбор количества
          компонент смеси используя алгоритм из лекции, но непонятно было
          по каким критриям оценивать параметры $m_0$ и $R$.

    \item Эвристика подбора количества компонент была изменена на 
          эвристику основанную на "взаимном проникновении" (mutual information)
          компонент. По мотивам \href{http://www.google.ru/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&ved=0CDEQFjAA&url=http%3A%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fdownload%3Fdoi%3D10.1.1.109.8192%26rep%3Drep1%26type%3Dpdf&ei=oXtTUeqCBcaS4ATEyoB4&usg=AF    QjCNFXjHdfDggeWS7WSAB5EAB731oluw&bvm=bv.44342787,d.bGE&cad=rjt}{вот этой статьи}.  Все протестировалось на ирисах. Работало отлично без нареканий.

    \item Был написан классификатор, также был протестирован на ирисах.

    \item Первый подход к датасету с цифрами и сразу 2 беды: нулевые дисперсии
          и маленькие значения плотности, при перемножении точности R не
          хватало и они вовсе становились 0, как следствие значения
          "временных переменных" на Е-шаге становились Inf и NaN.

    \item Было решено на шаге препроцессинга данных убирать параметры с нулевыми
          дисперсиями и нормализовывать парметры. Но это не решило проблемы
          с неправильными "временными переменными" из за слишком малой плотности.

    \item Было решено редуцировать размерность с помощью PCA. В итоге на шаге
          препроцессинга сначала для конкретного класса убирались парметры
          с нулевыми дисперсиями, потом оставшиеся параметры нормализовывались,
          после этого размерность уменьшалась с помощью PCA, и в конце новые
          полученные параметры снова нормализовывались.

    \item После всего этого стохастический метод определения очередного значения
          параметров на M-шаге для датасета всегда выдавал плохие параметры
          компонент, такие, что они были "взаимопроникаемыми", таким образом
          плотности всех классов всегда оценивались одной компонентой.

    \item Далее были эксперементы со случайным выбором новых параметров компонент,
          а также насильным задаванием числа компонент алгоритму.
\end{enumerate}


\subsection{Итоги.}
Узнал про эвристику с mutual information, вспомнил PCA, реализовал ЕM-алгоритм
с автоматическим подбором числа компонент. Но...

Плохо определяются еденицы, скорее всего это связано с тем, что я
отбрасываю свои параметры с нулевой дисперсией для каждого класса, а
для еденицы там много значимых нулей, когда у остальных цифр пятна, но
я так и не догадался как решить проблему с нулевыми дисперсиями по другому.
Скорее всего я где-то "пошел не по тому пути" потратил кучу времени,
получил грязный код и плохой классификатор.

В сухом остатке практическая часть оставила больше вопросов, чем ответов:
\begin{enumerate}
\item Как правильно делать препроцессинг данных?
\item Куда деть нулевые дисперсии?
\item Где я "пошел не туда"?
\end{enumerate}

\end{document}
