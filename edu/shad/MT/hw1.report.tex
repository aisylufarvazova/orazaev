\documentclass[12pt]{article}

\usepackage{amsfonts,amssymb}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage[dvips]{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage[ruled, lined]{algorithm2e}
\usepackage{hyperref}

\textheight=220mm
\textwidth=160mm

\newcommand{\sgn}{\operatorname{sgn}}
\newcommand{\argmax}{\operatorname{argmax}}
\newcommand{\NA}{\operatorname{NA}}
\newcommand{\OR}{\operatorname{ or }}
\newcommand{\LCS}{\operatorname{LCS}}
%\DeclareMathOperator{\sgn}{sgn}

\title{\bf Домашнее задание по курсу \\ <<Параллельные
и распределенные вычисления.>>}
\author{А.Е. Оразаев}
\date{}
\begin{document}

\voffset=-20mm
\hoffset=-12mm
\font\Got=eufm10 scaled\magstep2 \font\Got=eufm10

\maketitle

Лирическое отступление по поводу реализаций. Так получилось,
что я не смог присутствовать на 1-ом занятии и смотрел его в записи
прошлого года. Фактически как я посмотрел запись, я сразу приступил к
решению на Java. После этого появились записи курса этого года и к этому
времени у меня уже практически были готовы философы. Поэтому 1-ое задание
реализовано с помощью Java, второе с помощью C++.

\section{Обедающие философы.}
\paragraph{Разбираемся с дедлоком.} 
Первым делом было решено решить проблему с дедлоками следующим образом.
Каждой вилке присваивать уникальный порядковый "серийный номер" и брать
вилки в порядке возрастания серийных номеров. С этим связано некоторое
несоответствие отладочного вывода: 

\begin{verbatim}
   [N] took left fork.
\end{verbatim}

Вовсе не означает что философ взял именно левую вилку, в реализации
философ в этот момент берет меньшую по номеру из двух своих вилок.

Таким образом философы захватывают вилки-мьютексы всегда лишь в определенном
порядке, что исключает возможность дедлока.

\paragraph{Учим философов вежливости.}
Для того чтобы решить проблему "честности" ужина был придуман вежливый
режим для философов. Каждый филосов перед тем как приступить к трапезе
спрашивает у своих соседей сколько времени они всего голодали. Если это
время больше чем голодание текущего философа на 2 максимальных времени,
которое философ может думать, то философ "уступает" свое место соседям и
не подходит к столу случайное время от 0 до максимально времени, которое
философ может думать:
\begin{verbatim}
class RunnablePhilosopher {
    /* ... */
    private boolean bePolite(RunnablePhilosopher other) {
        if (other != null
            && other.getWaitTime() - getWaitTime() > maxWaitTime * 2) {
            tryToSleep(random.nextInt(maxWaitTime));
            return true;
        }

        return false;
    }
    /* ... */
}
\end{verbatim}

Также была попытка сделать "вежливый режим" не относительно времени голодания, а
отностительно количества съедаемой еды. Результаты эксперементов оценивались
по стандатному отклонению времения времени голодания философов.
Лучший результат показал "вежливый режим" относительно именно времени голодания.

Некоторые замеры:
\begin{verbatim}
5 философов, 60 секунд на ужин.
Максимальное время поедания и размышлений
"Вежливый режим" отключен.
    Philosopher 0 ate = 355 times and waited = 24181 ms.
    Philosopher 1 ate = 400 times and waited = 19521 ms.
    Philosopher 2 ate = 427 times and waited = 17530 ms.
    Philosopher 3 ate = 462 times and waited = 13751 ms.
    Philosopher 4 ate = 369 times and waited = 23036 ms.
    -------------- Summary ---------------
    Mean: 19603.0 ms.
    Stddev: 4221.774804510539.

5 философов, 60 секунд на ужин.
Максимальное время поедания и размышлений
"Вежливый режим" включен.
    Philosopher 0 ate = 376 times and waited = 21565 ms.
    Philosopher 1 ate = 385 times and waited = 21429 ms.
    Philosopher 2 ate = 389 times and waited = 21307 ms.
    Philosopher 3 ate = 386 times and waited = 21353 ms.
    Philosopher 4 ate = 385 times and waited = 21433 ms.
    -------------- Summary ---------------
    Mean: 21415.0 ms.
    Stddev: 98.14275317108238.
\end{verbatim}

В результате стандартное отклонение при "вежливом режиме" сильно
меньше, но немного выросло среднее. Это связано с тем
что у нас теперь нет философов, которые голодают сильно меньше
других, например как филосов 3, в первом замере.

\paragraph{Итоги.} Была решена проблема с дедлоком, а также философы стали
голодать примерно равное время, что делает ужин более честным.



\section{Поисковый робот.}
\paragraph{Дополнительные библиотеки.}
Для компиляции программы, кроме библиоткеи \verb|curl| также потребуются
библитеки \verb|htmlcxx| и \verb|boost| желательно версии выше чем 1.42.

Пример установки дополнительных библиотк для ubuntu:
\begin{verbatim}
$ sudo apt-get install libhtmlcxx-dev libboost1.42-all-dev
\end{verbatim}

\verb|htmlcxx| нужен для парсинга страницы, a \verb|boost| используются
для замеров времени, а также для нормализации и работы с URL-ами.

Обращю внимание, что в Makefile таже используется \verb|ccache| для ускорения
сборки. Поэтому нужен еще либо \verb|ccache|, либо его надо убрать из Makefile.

\paragraph{Сообщения потоков.}
Сразу было понятно, что нужно реализовывать очередь сообщений для потоков,
при этом сообщения, которые потоки могут обрабатывать должны быть нескольких
типов(\verb|TMessageType| в реализации):
\begin{itemize}
    \item \verb|T_GET| таск на выкачивания урла.
    \item \verb|T_PARSE| таск на парсинг html и добавление новых тасков
          на выкачивание.
    \item \verb|T_LOG| таск для логирования html на диск.
    \item \verb|T_POISON| сигнал потоку о том, что пора заканчивать.
\end{itemize}

Таски на логирование нужны были, потому что было принято решение о том, чтобы
писать все сообщения не в разные файлы, а в один большой файл \verb|pages.dump|,
путь до директории куда будет записываться этот файл передается в коммандной строке,
как и сказано в задании.

Так было решено потому:
\begin{itemize}
    \item решалась проблема с беспорядочным именованием файлов
    \item можно выделить отдельный поток, который будет заниматься только
          последовательной записью на диск
    \item не надо постоянно обращаться к диску из потоков, которые заняты парсингом.
          Хранить все в оперативной памяти не получится либо из-за больших объемов,
          либо из-за циклического буффера(но об этом ниже).
\end{itemize}

У каждого сообщения есть его глубина, чтобы понимать на какой глубине web-графа мы
находимся.


\paragraph{Очереди.}
Всего есть три очереди:
\begin{itemize}
    \item lock-based циклический буффер -- очередь закачки.
    \item lock-based очередь -- очередь для парсинга.
    \item lock-based очередь -- очередь для записи страниц на диск.
\end{itemize}

Изначально был вариант когда было лишь две очереди: для записи на диск(чтоб одним
главным потоком) и для тасков на скачивание и парсинг.

Осознав, что дольше всего работают таски именно скачивающие страницы, было решено попробовать
сделать две очереди: одну для скачивания страниц, другую для главного потока, который и
парсил и дампил страницы.

Эти вариант сильно быстро заполнял очередь для скачивания, благодоря тому, что
парсер добовлял очень много тасков на скачивание, на моем ноутбуке начинались беды с
пожиранием оперативной памяти.

В итоге решено это было циклическим буффером для очереди закачивания, но пришлось разделить
парсинг и дампинг в главный и один дополнительный поток, потому что если парсер не мог
добавить в переполненный циклический буффер тасков на закачку, то он честно ждал. А в это
время росла очередь на логирование. Можно было бы попробовать сделать что-то вроде try-insert
в циклический буффер, но времени уже не осталось эксперементировать.


\paragraph{Не качаем один URL много раз. Множество урлов.}
Для того, чтобы не качать одну и ту же страницу несколько раз, нужно ее
определенным образом нормализованный URL хранить в множестве и перед тем, как
добавлять очередной таск в очередь закачки, надо проверять добавляли мы уже
таск с таким URL-ом в очередь закачки.


\paragraph{Потоки.}
В итоге у нас есть несколько потоков:
\begin{itemize}
    \item Главный поток, который дампит страницы в \verb|pages.dump|.
    \item Один поток парсер, которого вполне хватает, чтобы справиться
          со всеми поступающими тасками на парсинг. Так как закачка это
          самое узкое место.
    \item Несколько потоков, которые занимаются загрузкой страниц. В реализации
          их 4.
\end{itemize}

Именно при 4-ех потоках загрузчиках я получал лучшие результаты. К сожелению, я
никак не смог сопоставить это значение с \verb|std::thread::hardware_concurrency()|,
ибо у меня на машине оно равно 0.

\paragraph{Окружение потоков.} Для облегчения совместной работы потоков 
была придумана абстракция окружения потока, которая хранит необходимые данные
для работы потоков:
\begin{itemize}
    \item Указатель на очередь тасков для потока
    \item Указатель на очередь результатов потока.
    \item Указатель на очередь для основного потока записывающего страницы на
          диск.
    \item Указатель на множество урлов, таски для которых уже были добавлены
          в очередь закачки.
    \item Максимальная глубина закачки
    \item Максимальное число страниц которое можно закачать.
    \item Указатель на объект-загрузчик страниц
    \item другие служебные данные.
\end{itemize}

Очередью тасков для потоков-загрузчиков является циклический буффер закачки,
а очередью резултатов -- очередь на парсинг.
В свою очередь(извиняюсь за каламбур), для потока парсера очередью тасков является
очередь на парсинг, а очередью резултатов -- циклический буффер закачки страниц.
А основному потоку нужна только очередь для логгинга -- его очередь тасков.

По сути потоки отличаются только своим окружением. То есть кем будет поток:
парсером, загрузчиком или дампером зависит только от того как расставлены указатели
на очереди. Это возможно благодоря единому классу обработки сообщений.

\paragraph{Цикл жизни страницы.}
Таким образом со URL-ом происходит следующее.
\begin{itemize}
    \item Парсер находит его на странице и добавляет его в очередь на закачку, если
          такого урла еще не было.
    \item Поток загрузчик достает таск на загрузку урла и выкачивает его.
    \item Поток загрузчик создает таск на парсинг страницы кторую загрузил и кладет
          в очередь на парсинг.
    \item Поток загрузчик создает таск на запись страницы в \verb|pages.dump|.
    \item Поток парсер достает таск на парсинг из очереди, парсит страницу и добавляет
          ее урлы в очередь закачки.
    \item Основной поток достает таск на дампинг страницы и пишет ее в \verb|pages.dump|.
\end{itemize}

\paragraph{Условие остановки.}
Когда множество урлов, для которых таски на закачку уже были добавлены в буффер закачки,
становится размера равного максимальному число странц, парсер перестает добавлять
таски на закачку в буффер закачки.
В свою очередь потоки-загрузчики перестают добавлять в очередь на парсинг сообщения
с глубиной большей, чем максимальная глубина web-графа.

За моментом, когда послать всем потокам "отравленные" сообщения следит основной поток.
Он ждет, когда опустошаться все очереди, а таже число записанных в \verb|pages.dump|
страниц будет равно числу урлов в множестве урлов, а также глубина достигнет максимальной
или число записанных на диск страниц будет равно максимальному числу страниц.

\paragraph{Итоги.}
Замеры производились на машине с вот такими характеристиками, которые можно
посмотреть \href{http://market.yandex.ru/model-spec.xml?CMD=-RR=9,0,0,0-PF=1801946~EQ~sel~1871523-PF=2142398356~EQ~sel~535392919-PF=1801946~EQ~sel~1871523-PF=2142398356~EQ~sel~535392919-PF=2142398543~EQ~sel~x1670617603-PF=2142398534~LT~sel~15.41-PF=2142398534~GT~sel~13.03-PF=2142398506~TR~sel~select-VIS=478-CAT_ID=432460-BPOS=10-EXC=1-PG=10&modelid=6297228&hid=91013}{здесь}.

Результаты для однопоточного поискового робота и многопоточного с 4-мя
потоками загрузчиками. Также приведена информация сколько по времени "качал"
каждый тред:
\begin{verbatim}
Однопоточный поисковый робот.(simple_crawler)
Сайт http://silikatsemey.kz/
Максимальное число загружаемых страниц: 1000
Максимальная глубина: 10 (Большая глубина чтобы выкачать первые 1000 страниц.)
    [0] Total downloading time for this thread is 564.549000 sec.
    real    9m25.932s
    user    0m1.744s
    sys     0m1.096s

Многопоточный поисковый робот.(crawler)
Сайт http://silikatsemey.kz/
Максимальное число загружаемых страниц: 1000
Максимальная глубина: 10 (Большая глубина чтобы выкачать первые 1000 страниц.)
    [0] Total downloading time for this thread is 130.391000 sec.
    [2] Total downloading time for this thread is 126.810000 sec.
    [3] Total downloading time for this thread is 126.881000 sec.
    [1] Total downloading time for this thread is 127.075000 sec.
    real    2m11.090s
    user    0m1.732s
    sys     0m1.204s

\end{verbatim}


\paragraph{Краткое описание основных файлов и классов.} Для простоты проверки
привожу краткую свотку основных файлов.
\begin{itemize}
    \item \verb|src/crawler.h| файл с описанием однопоточного и многопоточного робота.
    \begin{itemize}
        \item \verb|TCrawler| основной класс поискового робота.
        \item \verb|TSimpleCrawler| класс однопоточного поискового робота нужного лишь
              для того, чтобы знать насколько в основном варианте все лучше.
        \item \verb|TCrawlerConfiguration| обертка для конфигурационных параметров для
              \verb|TCrawler|.
    \end{itemize}

    \item \verb|src/messages.h| описание класса сообщений, а также окружения потока.
    \begin{itemize}
        \item \verb|TTaskMessage| класс сообщения для потока.
        \item \verb|TWorkerEnvironment| класс обертка для данных потока (окружение потока).
    \end{itemize}

    \item \verb|src/queue.h| описание thread-safe очередей нужных роботу, с общим интерфейсом
          \verb|TQueue|.
    \item \verb|src/set.h| описание однопоточного и многопоточного hash-set с общим интерфейсом
          \verb|TSet|.

    \item \verb|src/message_processor.h| описание обработчиков сообщений разных типов.
    \item \verb|src/worker.h| функтор для std::thread.
    \item \verb|src/page_downloader.h| описание загрузчика страниц.
    \begin{itemize} 
        \item \verb|TPageDownloader| класс обертка над \verb|curl| для выкачивания страниц,
              и сбора попутной информации (сколько времени скачивалось, сколько всего скачивалось
              и тд.)
    \end{itemize}

    \item \verb|src/link_parser.h| Классы для работы с URL-ами, а также парсинга html.
          Работа с url-ами реализована через \verb|boost::regex| не самым прямым способом.

\end{itemize}


\end{document}
